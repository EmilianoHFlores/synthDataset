# -*- coding: utf-8 -*-
"""YOLO_SAM_autolabel_multiple.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v1iu8aQ-F8uVBcj8-fXuRE6_s1urObMa
"""
# Commented out IPython magic to ensure Python compatibility.
# %cd segment-anything/notebooks
import os
import time
import torch
import cv2
import os
import matplotlib.pyplot as plt
import numpy as np
from pathlib import Path

startTime = time.time()
# Model
model = torch.hub.load("ultralytics/yolov5", "yolov5x")  # or yolov5n - yolov5x6, custom


import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor
sam_model = "h"

#wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth
if sam_model =="h":
  sam_checkpoint = "sam_vit_h_4b8939.pth"
  model_type = "vit_h"
else:
  sam_checkpoint = "sam_vit_l_0b3195.pth"
  model_type = "vit_l"

device = "cuda"

sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)

predictor = SamPredictor(sam)

images=[]
annotations=[]
categories=[]

image_0 ={"id": 0, "file_name": "0.jpg", "height": 480, "width": 736}

categoryDict = {0 : "Sal"}

img_id=0
anno_id=0

pathToAnnotate = 'DatasetCasa1'
imgPaths = os.listdir(pathToAnnotate)
print(imgPaths)

for imgPath in imgPaths:
  print(f"Processing image: {imgPath}")
  img = cv2.imread(f"{pathToAnnotate}/{imgPath}")
  if img is None:
    continue
  results = model(img)

  image_bboxes = []
  xywh = []

  #Get yolo results
  for *xyxy, conf, cls in results.pandas().xyxy[0].itertuples(index=False):
    #run for each detection
    if cls in ['cup', 'bottle', 'vase']:
      category_id = 0
      print(f"Predicted {cls} at {[round(elem, 2) for elem in xyxy ]} with on img {imgPath}.")
      image_bbox = (np.array([xyxy[0], xyxy[1], xyxy[2], xyxy[3]]))
      xywh = [xyxy[0], xyxy[1], xyxy[2]-xyxy[0], xyxy[3]-xyxy[1]]

      predictor.set_image(img)
  
      mask, _, _ = predictor.predict(
          point_coords=None,
          point_labels=None,
          box=image_bbox,
          multimask_output=False,
      )

      contours, _ = cv2.findContours(mask[0].astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
      cv2.drawContours(img, contours, -1, (255,0,255), 3)

      segmentation = []
      aux_segmentation = []
      for contour in contours:
        polygon = contour.flatten().tolist()
        if len(polygon) % 2 != 0:
            print("Error: polygon has an odd number of coordinates")
            continue
        polygon = np.array(polygon).reshape((-1, 2))
        aux_segmentation.append(polygon)
      polygon_area = int(cv2.contourArea(aux_segmentation[0]))

      for sublist in aux_segmentation[0]:
        for item in sublist:
          segmentation.append(int(item))
      
      annotations.append({"id": anno_id,"image_id": img_id,"category_id": category_id,"bbox": [int(xywh[0]), int(xywh[1]), int(xywh[2]), int(xywh[3])],"segmentation": segmentation,"area": polygon_area,"iscrowd": 0})
      anno_id = anno_id+1
  print(f"Imgid: {img_id}")
  images.append({"id": img_id, "file_name": imgPath,"height": img.shape[0],"width": img.shape[1]})
  img_id = img_id+1
  print(f"Finished image {imgPath}")

  plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
  plt.show()

categories = []
for id in categoryDict:
  categories.append({"id": id, "name": categoryDict[id]})
#print(annotations)

# Define the COCO dictionary
coco_dict = {
    "images": images,
    "annotations": annotations,
    "categories": categories
}

print(coco_dict)

import json
json_annotation = json.dumps(coco_dict)
with open('annotations.json', 'w') as f:
  f.write(json_annotation)

print(f"Total time: {time.time()-startTime}")
